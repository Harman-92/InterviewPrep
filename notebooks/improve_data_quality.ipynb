{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd  \n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating directory to store dataset\n",
    "if not os.path.isdir(\"../data/transport\"):\n",
    "    os.makedirs(\"../data/transport\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying gs://cloud-training-demos/feat_eng/transport/untidy_vehicle_data.csv...\n",
      "/ [0 files][    0.0 B/ 47.2 KiB]                                                \n",
      "-\n",
      "- [1 files][ 47.2 KiB/ 47.2 KiB]                                                \n",
      "\n",
      "Operation completed over 1 objects/47.2 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "# Download the raw .csv data by copying the data from a cloud storage bucket.\n",
    "!gsutil cp gs://cloud-training-demos/feat_eng/transport/untidy_vehicle_data.csv ../data/transport"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Dataset into a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222
    },
    "colab_type": "code",
    "id": "REZ57BXCLdfG",
    "outputId": "a6ef2eda-c7eb-4e2d-92e4-e7fcaa20b0af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "        Date Zip Code Model Year                      Fuel       Make  \\\n0 2018-10-01    90000       2006                  Gasoline  OTHER/UNK   \n1 2018-10-01      NaN       2014                  Gasoline        NaN   \n2        NaT    90000        NaN                  Gasoline  OTHER/UNK   \n3 2018-10-01    90000       2017                  Gasoline  OTHER/UNK   \n4 2018-10-01    90000      <2006  Diesel and Diesel Hybrid  OTHER/UNK   \n\n  Light_Duty  Vehicles  \n0        NaN       1.0  \n1        Yes       1.0  \n2        Yes       NaN  \n3        Yes       1.0  \n4         No      55.0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>Zip Code</th>\n      <th>Model Year</th>\n      <th>Fuel</th>\n      <th>Make</th>\n      <th>Light_Duty</th>\n      <th>Vehicles</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2018-10-01</td>\n      <td>90000</td>\n      <td>2006</td>\n      <td>Gasoline</td>\n      <td>OTHER/UNK</td>\n      <td>NaN</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2018-10-01</td>\n      <td>NaN</td>\n      <td>2014</td>\n      <td>Gasoline</td>\n      <td>NaN</td>\n      <td>Yes</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>NaT</td>\n      <td>90000</td>\n      <td>NaN</td>\n      <td>Gasoline</td>\n      <td>OTHER/UNK</td>\n      <td>Yes</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2018-10-01</td>\n      <td>90000</td>\n      <td>2017</td>\n      <td>Gasoline</td>\n      <td>OTHER/UNK</td>\n      <td>Yes</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2018-10-01</td>\n      <td>90000</td>\n      <td>&lt;2006</td>\n      <td>Diesel and Diesel Hybrid</td>\n      <td>OTHER/UNK</td>\n      <td>No</td>\n      <td>55.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_transport = pd.read_csv('../data/transport/untidy_vehicle_data.csv',parse_dates=['Date'])\n",
    "\n",
    "# Output the first five rows.\n",
    "df_transport.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional - Create a report using pandas-profiling\n",
    "\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "report = ProfileReport(df_transport)\n",
    "report.to_file(output_file='../output/Vehicles.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional - Do on the fly operations on the dataset and create visualizations using dtale\n",
    "\n",
    "import dtale \n",
    "\n",
    "dtale.show(df_transport).open_browser()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame Column Data Types\n",
    "\n",
    "DataFrames may have heterogenous or \"mixed\" data types, that is, some columns are numbers, some are strings, and some are dates etc. Because CSV files do not contain information on what data types are contained in each column, Pandas infers the data types when loading the data, e.g. if a column contains only numbers, Pandas will set that column’s data type to numeric: integer or float.\n",
    "\n",
    "Run the next cell to see information on the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 999 entries, 0 to 998\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count  Dtype         \n",
      "---  ------      --------------  -----         \n",
      " 0   Date        997 non-null    datetime64[ns]\n",
      " 1   Zip Code    997 non-null    object        \n",
      " 2   Model Year  997 non-null    object        \n",
      " 3   Fuel        996 non-null    object        \n",
      " 4   Make        996 non-null    object        \n",
      " 5   Light_Duty  996 non-null    object        \n",
      " 6   Vehicles    996 non-null    float64       \n",
      "dtypes: datetime64[ns](1), float64(1), object(5)\n",
      "memory usage: 54.8+ KB\n"
     ]
    }
   ],
   "source": [
    "# The .info() function will display the concise summary of an dataframe.\n",
    "df_transport.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 5 string objects, one datetime and one float object. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use .describe() to see some summary statistics for the numeric fields (Vehicles)\n",
    "df_transport.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describing all columns of a DataFrame of object datatype.\n",
    "df_transport.describe(include=[object])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describing all columns of a DataFrame of datetime datatype.\n",
    "df_transport.describe(exclude = [np.number,object],datetime_is_numeric=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's investigate a bit more of our data by using the .groupby() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The .groupby() function is used for spliting the data into groups based on some criteria.\n",
    "grouped_data = df_transport.groupby(['Zip Code','Model Year','Fuel','Make','Light_Duty','Vehicles'])\n",
    "\n",
    " # Get the first entry for each month.\n",
    "df_transport.groupby('Fuel').first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for Missing Values\n",
    "\n",
    "Missing values adversely impact data quality, as they can lead the machine learning model to make inaccurate inferences about the data. Missing values can be the result of numerous factors, e.g. \"bits\" lost during streaming transmission, data entry, or perhaps a user forgot to fill in a field.  Note that Pandas recognizes both empty cells and “NaN” types as missing values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's show the null values for all features in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transport.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see a sampling of which values are missing, enter the feature column name.  You'll notice that \"False\" and \"True\" correpond to the presence or abscence of a value by index number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_transport[df_transport['Date'].isnull()])\n",
    "print(df_transport[~df_transport['Date'].notna()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What can we deduce about the data at this point?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's summarize our data by row, column, features, unique, and missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python shape() is used in pandas to give the number of rows/columns.\n",
    "# The number of rows is given by .shape[0]. The number of columns is given by .shape[1].\n",
    "# Thus, shape() consists of an array having two arguments -- rows and columns\n",
    "print (\"Rows     : \" ,df_transport.shape[0])\n",
    "print (\"Columns  : \" ,df_transport.shape[1])\n",
    "print (\"\\nFeatures : \\n\" ,df_transport.columns.tolist())\n",
    "print (\"\\nUnique values :  \\n\",df_transport.nunique())\n",
    "print (\"\\nMissing values :  \", df_transport.isnull().sum().values.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transport['Zip Code'].value_counts().reset_index(name='Zip Count').rename(columns={'index':'Zip Code'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the data again -- this time the last five rows in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the last five rows in the dataset.\n",
    "df_transport.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Are Our Data Quality Issues?\n",
    "\n",
    "1. **Data Quality Issue #1**:  \n",
    "> **Missing Values**:\n",
    "Each feature column has multiple missing values.  In fact, we have a total of 18 missing values.\n",
    "  \n",
    "2. **Data Quality Issue #2**: \n",
    "> **Model Year**: We are only interested in years greater than 2006, not \"<2006\".\n",
    "3. **Data Quality Issue #3**:  \n",
    "> **Categorical Columns**:  The feature column \"Light_Duty\" is categorical and has a \"Yes/No\" choice.  We cannot feed values like this into a machine learning model.  In addition, we need to \"one-hot encode the remaining \"string\"/\"object\" columns.\n",
    "4. **Data Quality Issue #4**:  \n",
    "> **Temporal Features**:  How do we handle year, month, and day?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Quality Issue #1:  \n",
    "##### Resolving Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most algorithms do not accept missing values. \n",
    "\n",
    "One option is to \"drop all the rows\" with missing values.  \n",
    "\n",
    "Lets explore other options.\n",
    "\n",
    "For numeric columns, we can use  \"mean\", \"mode\" and \"median\" values to fill in the missing numeric values depending on the column data.\n",
    "\n",
    "For categorical columns, use the \"mode\" (or most frequent values) to fill in missing categorical values. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check again for missing values by showing how many rows contain NaN values for each feature column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The isnull() method is used to check and manage NULL values in a data frame.\n",
    "\n",
    "df_transport.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell to apply the lambda function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets replace every column value with its mode i.e. most frequent value\n",
    "\n",
    "df_transport = df_transport.apply(lambda x:x.fillna(x.value_counts().index[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check again for missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Date          2\nZip Code      2\nModel Year    2\nFuel          3\nMake          3\nLight_Duty    3\nVehicles      3\ndtype: int64"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_transport.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have Dates as a integers, let's do some additional plotting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Quality Issue #2:  \n",
    "##### Rename a Feature Column and Remove a Value.  \n",
    "\n",
    "Our feature columns have different \"capitalizations\" in their names, e.g. both upper and lower \"case\".  In addition, there are \"spaces\" in some of the column names.  In addition, we are only interested in years greater than 2006, not \"<2006\".  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also resolve the \"case\" problem too by making all the feature column names lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's remove all the spaces for feature columns by renaming them.\n",
    "\n",
    "df_transport.rename(columns = { 'Date': 'date', 'Zip Code':'zipcode', 'Model Year': 'modelyear', 'Fuel': 'fuel', 'Make': 'make', 'Light_Duty': 'lightduty', 'Vehicles': 'vehicles'}, inplace = True) \n",
    "\n",
    "# Output the first two rows.\n",
    "df_transport.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a copy of the dataframe to avoid copy warning issues.\n",
    "\n",
    "df = df_transport.loc[df_transport.modelyear != '<2006'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm that the modelyear value '<2006' has been removed by doing a value count.\n",
    "df['modelyear'].value_counts(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Quality Issue #3:  \n",
    "##### Handling Categorical Columns\n",
    "\n",
    "The feature column \"lightduty\" is categorical and has a \"Yes/No\" choice.  We cannot feed values like this into a machine learning model.  We need to convert the binary answers from strings of yes/no to integers of 1/0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets count the number of \"Yes\" and\"No's\" in the 'lightduty' feature column.\n",
    "df['lightduty'].value_counts(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's convert the Yes to 1 and No to 0.\n",
    "# The .apply takes a function and applies it to all values of a Pandas series (e.g. lightduty). \n",
    "df.loc[:,'lightduty'] = df['lightduty'].apply(lambda x: 0 if x=='No' else 1)\n",
    "df['lightduty'].value_counts(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm that \"lightduty\" has been converted.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-Hot Encoding Categorical Feature Columns\n",
    "\n",
    "Machine learning algorithms expect input vectors and not categorical features. Specifically, they cannot handle text or string values.  Thus, it is often useful to transform categorical features into vectors.\n",
    "\n",
    "One transformation method is to create dummy variables for our categorical features.  Dummy variables are a set of binary (0 or 1) variables that each represent a single class from a categorical feature.  We simply  encode the categorical variable as a one-hot vector, i.e. a vector where only one element is non-zero, or hot.  With one-hot encoding, a categorical feature becomes an array whose size is the number of possible choices for that feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making dummy variables for categorical data with more inputs.  \n",
    "data_dummy = pd.get_dummies(df[['zipcode','modelyear', 'fuel', 'make']], drop_first=True)\n",
    "\n",
    "# Output the first five rows.\n",
    "data_dummy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging (concatenate) original data frame with 'dummy' dataframe.\n",
    "\n",
    "df = pd.concat([df,data_dummy], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping attributes for which we made dummy variables.  Let's also drop the Date column.\n",
    "\n",
    "df = df.drop(['date','zipcode','modelyear', 'fuel', 'make'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm that 'zipcode','modelyear', 'fuel', and 'make' have been dropped.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Quality Issue #4:  \n",
    "##### Temporal Feature Columns\n",
    "\n",
    "Lets parse date to create year,month and day feature columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will parse Date into three columns that is year, month, and day.\n",
    "df_transport['year'] = df_transport['Date'].dt.year\n",
    "df_transport['month'] = df_transport['Date'].dt.month\n",
    "df_transport['day'] = df_transport['Date'].dt.day\n",
    "\n",
    "# The .info() function will display the concise summary of an dataframe.\n",
    "df_transport.info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we are creating a new dataframe called \"grouped_data\" and grouping by on the column \"Make\"\n",
    "grouped_data = df_transport.groupby(['Make'])\n",
    "\n",
    "# Get the first entry for each month.\n",
    "df_transport.groupby('month').first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we will visualize our data using the figure() function in the pyplot module of matplotlib's library -- which is used to create a new figure.\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "# Seaborn's .jointplot() displays a relationship between 2 variables (bivariate) as well as 1D profiles (univariate) in the margins. This plot is a convenience class that wraps JointGrid.\n",
    "sns.jointplot(x='month',y='Vehicles',data=df_transport)\n",
    "\n",
    "# The title() method in matplotlib module is used to specify title of the visualization depicted and displays the title using various attributes.\n",
    "plt.title('Vehicles by Month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's print the unique values for \"month\", \"day\" and \"year\" in our dataset. \n",
    "print ('Unique values of month:',df.month.unique())\n",
    "print ('Unique values of day:',df.day.unique())\n",
    "print ('Unique values of year:',df.year.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now convert the month and day feature columns to meaningful representations as a way to get us thinking about changing temporal features -- as they are sometimes overlooked.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we map each temporal variable onto a circle such that the lowest value for that variable appears right next to the largest value. We compute the x- and y- component of that point using the sin and cos trigonometric functions.\n",
    "df['day_sin'] = np.sin(df.day*(2.*np.pi/31))\n",
    "df['day_cos'] = np.cos(df.day*(2.*np.pi/31))\n",
    "df['month_sin'] = np.sin((df.month-1)*(2.*np.pi/12))\n",
    "df['month_cos'] = np.cos((df.month-1)*(2.*np.pi/12))\n",
    "\n",
    "# Let's drop month, and day\n",
    "# TODO 5\n",
    "df = df.drop(['month','day','year'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scroll left to see the converted month and day coluumns.\n",
    "df.tail(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "We resolved missing values, converted the Date feature column to a datetime format, renamed feature columns, removed a value from a feature column, created one-hot encoding features, and converted temporal features to meaningful representations.  "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Basic Feature Engineering in Keras.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}